@article{{huxu2021videoclip},
    title={VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text
  Understanding},
    url={http://arxiv.org/abs/2109.14084v2},
    author={Hu Xu and Gargi Ghosh and Po-Yao Huang and Dmytro Okhonko and Armen Aghajanyan and Florian Metze and Luke Zettlemoyer and Christoph Feichtenhofer},
    abstract={We present VideoCLIP, a contrastive approach to pre-train a unified model for zero-shot video and text understanding, without using any labels on downstream tasks. VideoCLIP trains a transformer for video and text by contrasting temporally overlapping positive video-text pairs with hard negatives from nearest neighbor retrieval. Our experiments on a diverse series of downstream tasks, including sequence-level text-video retrieval, VideoQA, token-level action localization, and action segmentation reveal state-of-the-art performance, surpassing prior work, and in some cases even outperforming supervised approaches. Code is made available at https://github.com/pytorch/fairseq/tree/main/examples/MMPT.},
    month={9},
    year={2021}
}
